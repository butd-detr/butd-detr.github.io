<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds">
  <meta name="keywords" content="BUTD-DETR, 3D language grounding, 2D Language grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <article class="post-content">
    <meta name="twitter:title" content="Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds"/>
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:image" content="https://butd-detr.github.io/static/images/teaser.png" />
  <article class="post-content">

  <title>Bottom Up and Top Down Detection Transformers for Language Grounding in Images and Point Clouds</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
 <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds</h1>
          <div class="is-size-4 publication-authors">
            <span class="author-block">
              <a href="https://ayushjain1144.github.io/">Ayush Jain</a><sup>*</sup><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://nickgkan.github.io/">Nikolaos Gkanatsios</a><sup>*</sup><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ishita.io/">Ishita Mediratta</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a><sup>1</sup>
            </span>
            
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>2</sup>Meta AI</span>
          </div>
          
          <div class="is-size-5 contribution">
            <span class="appear"><a href="https://eccv2022.ecva.net/">ECCV 2022</a></span>
          </div>

          <div class="is-size-5 contribution">
            <span class="contribution"><sup>*</sup>Equal Contribution (God may not play dice, we do)</span>
          </div><br> 

          <table align=center width=700px>
            <tr>
              <td align=center width=100px><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2112.08879">[Paper]</a></span></center></td>
              <td align=center width=100px><center><span style="font-size:28px"><a href="https://github.com/nickgkan/butd_detr">[Code]</a></span></center></td>
          <tr/>
        </table>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Most models tasked to ground referential utterances in 2D and 3D scenes learn to select the referred object from a pool of object proposals provided by a pre-trained detector. This is limiting because an utterance may refer to visual entities at various levels of granularity, such as the chair, the leg of the chair, or the tip of the front leg of the chair, which may be missed by the detector. We propose a language grounding model that attends on the referential utterance and on the object proposal pool computed from a pre-trained detector to decode referenced objects with a detection head, without selecting them from the pool. In this way, it is helped by powerful pre-trained object detectors without being restricted by their misses. We call our model Bottom Up Top Down DEtection TRansformers (BUTD-DETR) because it uses both language guidance (top down) and objectness guidance (bottom-up) to ground referential utterances in images and point clouds. Moreover, BUTD-DETR casts object detection as referential grounding and uses object labels as language prompts to be grounded in the visual scene, augmenting supervision for the referential grounding task in this way. The proposed model sets a new state-of-the-art across popular 3D language grounding benchmarks with significant performance gains over previous 3D approaches (12.6% on SR3D, 11.6% on NR3D and 6.3% on ScanRefer). When applied in 2D images, it performs on par with the previous state of the art. We ablate the design choices of our model and quantify their contribution to performance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video.
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/SM4seQgf9n4?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    / Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"> Method: BUTD-DETR (pronounced Beauty-DETR)</h2>
        
        <div class="col-md-8 col-md-offset-2">
                <figure>
                    <img src="./static/images/teaser.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                    <!-- <figcaption class="figure-caption text-center" style="font-size: 14px;">
                        <b>Fig. 5.</b> The ReferIt3DNet neural listener.
                    </figcaption> -->
                </figure>
    </div>
        
        <div class="content has-text-justified">
          <p>
            Given a 3D scene and a language referential utterance, the model localizes all object instances mentioned in the
	    utterance. A pre-trained object detector extracts object box proposals. The 3D point cloud, the language utterance and the labelled box proposals are encoded
	   into corresponding sequences of point, word and box tokens using visual, language and box encoders, respectively. The three streams cross-attend and finally
	   decode boxes and corresponding spans in the language utterance that the decoded box refers to.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"> Quantitative Results </h2>
       
        <div class="content has-text-justified">
          <p>
            BUTD-DETR achieves state-of-the-art performance on 3D language grounding, outperforming all previous models as well as an MDETR-3D equivalent that we implemented.
          </p>
        </div>

        <div class="col-md-8 col-md-offset-2">
                <figure>
                    <img src="./static/images/performance.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                    <!-- <figcaption class="figure-caption text-center" style="font-size: 14px;">
                        <b>Fig. 5.</b> The ReferIt3DNet neural listener.
                    </figcaption> -->
                </figure>
    </div>
    <div class="content has-text-justified">
      <p>
        BUTD-DETR works on 2D domain as well achieving comparable performance to MDETR, while converging twice as fast, due to architectural optimizations such as deformable attention. With minimal changes, our model works both on 3D and 2D, thus taking a step towards unifying grounding models for 2D and 3D.
      </p>
    </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"> Qualitative Results: 3D </h2>
        
        <div class="col-md-8 col-md-offset-2">
                <figure>
                    <img src="./static/images/qual.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                    <figcaption class="figure-caption text-center" style="font-size: 24px;">
                    <span style="color: #0000a0">Blue boxes</span>
 refer to our predictions,
  <span style="color: #00ff00">green boxes</span> 
                           refer to ground truths.
                    </figcaption>
                </figure>
    </div>
        
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"> Qualitative Results: 2D </h2>
        
        <div class="col-md-8 col-md-offset-2">
                <figure>
                    <img src="./static/images/qual_2d.png" style="padding-bottom:10px" class="img-responsive" alt="overview">
                    <figcaption class="figure-caption text-center" style="font-size: 24px;">
                    <span style="color: #ff0000">Red boxes</span>
 refer to our predictions,
  <span style="color: #00ff00">green boxes</span> 
                           refer to ground truths.
                    </figcaption>
                </figure>
    </div>
        
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX1">
<div class="container is-max-desktop content">
  <h2 class="title" align="center">Paper and Bibtex</h2>

    <span class="gg">
      <a href="https://arxiv.org/abs/2112.08879"><img style="width:250px" src="static/images/thumbnail.png" align="left"/> </a>
    
   
      <!-- <a href="htps://arxiv.org/abs/2112.08879">[Paper]</a> -->
    </span>
    <pre><code>
      @misc{https://doi.org/10.48550/arxiv.2112.08879,
        doi = {10.48550/ARXIV.2112.08879},
        url = {https://arxiv.org/abs/2112.08879},
        author = {Jain, Ayush and Gkanatsios, Nikolaos and Mediratta, Ishita and Fragkiadaki, Katerina},
        keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds},
        publisher = {arXiv},
        year = {2021},
        copyright = {Creative Commons Attribution 4.0 International}
      }
    </code></pre>

    <!-- <span class="author-block">
      <a href="https://arxiv.org/abs/2112.08879"><img style="width:400px" src="static/images/thumbnail.png"/></a> 
      <a href="https://arxiv.org/abs/2112.08879">[Paper]</a>
    </span> -->

  <!-- <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Jain, A., Gkanatsios, N., Mediratta, I. and Fragkiadaki, K. (2021). <br>
      Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds
      <br> <em> ArXiv, abs/2112.08879.</em> </span></p> -->
  <!-- <p style="margin-top:20px;"></p> -->     
  </div>
</section>
<br><hr>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2112.08879">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/nickgkan/butd_detr" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Source code of this website is borrowed from  <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies template</a>.
            
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
